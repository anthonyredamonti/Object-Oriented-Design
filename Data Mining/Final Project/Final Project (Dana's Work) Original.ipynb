{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74502cc0",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "---------------------------------------------------------------------\n",
    "## CIS 600 Fundamental Data & Knowledge Mining\n",
    "## Prof. Ying Lin\n",
    "## 12/3/2022\n",
    "\n",
    "### Anthony Redamonti, Dana Dippery, Joshua, Hal Baird\n",
    "### Syracuse University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b72911b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #, recall_score\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Not using, should probably delete:\n",
    "# import re\n",
    "# import random\n",
    "# import nltk\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24ea8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time using nltk, need to download data!\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8af1e",
   "metadata": {},
   "source": [
    "# Get Data from All Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44377cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: DataSets\\httpsarchive.ics.uci.edu\\ml\\machine-learning-databases\\00228\\SMSSpamCollection.csv\n",
      "\tContributed row count: 5158\n",
      "Source: DataSets\\www.kaggle.com_datasets\\arunasivapragasam\\spam-or-ham\\SMSCollection.csv\n",
      "\tContributed row count: 1\n",
      "Source: DataSets\\www.kaggle.com_datasets\\assumewisely\\sms-spam-collection\\SMSSpamCollection.csv\n",
      "\tContributed row count: 0\n",
      "Source: DataSets\\www.kaggle.com_datasets\\hdza1991\\sms-spam\\sms_spam.csv\n",
      "\tContributed row count: 80\n",
      "Source: DataSets\\www.kaggle.com_datasets\\kaushikmanjunatha\\dataset\\SMSSpamCollection.csv\n",
      "\tContributed row count: 0\n",
      "Source: DataSets\\www.kaggle.com_datasets\\lampubhutia\\email-spam-ham-prediction\\sms_spam.csv\n",
      "\tContributed row count: 0\n",
      "Source: DataSets\\www.kaggle.com_datasets\\nilaychauhan\\sms-spam-detection\\SMSSpamCollection.csv\n",
      "\tContributed row count: 0\n",
      "Source: DataSets\\www.kaggle.com_datasets\\shravan3273\\sms-spam\\spamraw.csv\n",
      "\tContributed row count: 645\n",
      "Source: DataSets\\www.kaggle.com_datasets\\shrutipandit707\\smsspamcollection\\smsspamcollection.csv\n",
      "\tContributed row count: 36\n",
      "Source: DataSets\\www.kaggle.com_datasets\\vivekchutke\\spam-ham-sms-dataset\\sms_spam.csv\n",
      "\tContributed row count: 0\n",
      "Total data rows: 5920\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# When processing data, check for:\n",
    "# - file ends with .csv - indicates it has been humanly preprocessed\n",
    "# - header removed\n",
    "# - first column is label, second is message\n",
    "# - labeld as ham/spam\n",
    "\n",
    "all_data = pd.DataFrame([], columns=[\"label\", \"message\"])\n",
    "\n",
    "def process_file(fp):\n",
    "    for sep in ['\\t', ',']:\n",
    "        for quote in [csv.QUOTE_ALL, csv.QUOTE_NONE, csv.QUOTE_MINIMAL]:\n",
    "            try:\n",
    "                d = pd.read_csv(fp, names=[\"label\", \"message\"], sep=sep, quoting=quote)\n",
    "                \n",
    "                # Make sure we processed the data correctly\n",
    "                # We can confirm by checking the labels... we should get exactly\n",
    "                #   2 unique labels, \"ham\" and \"spam\"\n",
    "                label_count = len(d[\"label\"].unique())\n",
    "                if label_count > 2:\n",
    "                    raise Exception(\"3+ labels found\")\n",
    "                label_set = sorted(list(d[\"label\"].unique()))\n",
    "                if label_set != [\"ham\", \"spam\"]:\n",
    "                    raise Exception(\"ham|spam not in labels\")\n",
    "                \n",
    "                # Trim the pre/post space off the message, to de-duplicate\n",
    "                d[\"message\"] = d[\"message\"].str.strip()\n",
    "                \n",
    "                # Return results\n",
    "                return d\n",
    "            \n",
    "            except:\n",
    "                # Ignore errors and try processing again, with different parameters\n",
    "                pass\n",
    "    \n",
    "    # If we none of the processing parameters work, then report it!\n",
    "    # All files have processed successfully, so we don't see this output.\n",
    "    print(f\"\\tFailed: {fp}\")\n",
    "\n",
    "# Find all data files (*csv) in the the DataSets directory\n",
    "# fp_all_data_out = os.path.join(\"DataSets\", \"all_data.csv\")\n",
    "for root, dirs, files in os.walk(\"DataSets\"):\n",
    "    for name in files:\n",
    "        if name.endswith('.csv'):\n",
    "            \n",
    "            # Process this file\n",
    "            fp = os.path.join(root, name)\n",
    "            print(f\"Source: {fp}\")\n",
    "            data = process_file(fp)\n",
    "            \n",
    "            # Report number of data rows this file contributes after de-duplicating\n",
    "            original_count = all_data.shape[0]\n",
    "            all_data = pd.concat([all_data, data], axis=0, ignore_index=True)\n",
    "            all_data.drop_duplicates(inplace=True, ignore_index=True)\n",
    "            print(f\"\\tContributed row count: {all_data.shape[0] - original_count}\")\n",
    "\n",
    "# Print summary data\n",
    "print(f\"Total data rows: {all_data.shape[0]}\")\n",
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2ce8b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ea712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac0fbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stem(document):\n",
    "    stemmer = PorterStemmer()\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_lemmatize(document):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_stem_and_lemmatize(document):\n",
    "    stemmer = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a974a",
   "metadata": {},
   "source": [
    "# Train and Analyze Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1812c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "Y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "444f2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_analyze(classifier):\n",
    "    '''Build, train and test a model based on the input classifier'''\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # Get classifier name\n",
    "    classifier_name = type(classifier).__name__\n",
    "\n",
    "    # Build training pipeline\n",
    "    vectorizer = TfidfVectorizer(min_df= 3, sublinear_tf=True, norm='l2', ngram_range=(1, 2),\n",
    "                                preprocessor=preprocess_stem)\n",
    "    pipeline = Pipeline([('vect', vectorizer),\n",
    "                         ('chi',  SelectKBest(chi2, k=1000)),\n",
    "                         ('to_dense', DenseTransformer()),\n",
    "                         ('clf', classifier)])\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    #pipeline.steps[3][1].feature_names = list(X_train.columns.values)\n",
    "    \n",
    "    # Test the model\n",
    "    y_preds = model.predict(X_test)\n",
    "    y_test_accuracy = accuracy_score(y_test, y_preds) * 100\n",
    "    \n",
    "    # Output the model scoring/accuracy\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    print(f\"Accuracy on test data: {round(y_test_accuracy, 2)}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1b724e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GaussianNB\n",
      "Accuracy on test data: 97.69\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1577\n",
      "        spam       0.90      0.89      0.90       199\n",
      "\n",
      "    accuracy                           0.98      1776\n",
      "   macro avg       0.94      0.94      0.94      1776\n",
      "weighted avg       0.98      0.98      0.98      1776\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1557   20]\n",
      " [  21  178]]\n"
     ]
    }
   ],
   "source": [
    "gnbPipeline = train_test_analyze(GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82ab39",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95ced145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: BernoulliNB\n",
      "Accuracy on test data: 97.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1577\n",
      "        spam       0.99      0.80      0.89       199\n",
      "\n",
      "    accuracy                           0.98      1776\n",
      "   macro avg       0.98      0.90      0.94      1776\n",
      "weighted avg       0.98      0.98      0.98      1776\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1576    1]\n",
      " [  39  160]]\n"
     ]
    }
   ],
   "source": [
    "bnbPipeline = train_test_analyze(BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0621961",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6c18fe1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: RandomForestClassifier\n",
      "Accuracy on test data: 97.86\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1577\n",
      "        spam       0.99      0.82      0.90       199\n",
      "\n",
      "    accuracy                           0.98      1776\n",
      "   macro avg       0.98      0.91      0.94      1776\n",
      "weighted avg       0.98      0.98      0.98      1776\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1575    2]\n",
      " [  36  163]]\n"
     ]
    }
   ],
   "source": [
    "rfcPipeline = train_test_analyze(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8ab94a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#param_grid_rfc = {'n_estimators': [50, 100, 150, 200],\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#             'criterion': ['gini', 'entropy'],\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#             'max_features': ['sqrt', 'log2', None]}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#rfcPipeline.steps[3][1].fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m feature_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rfcPipeline\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_, index\u001b[38;5;241m=\u001b[39m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m,\n\u001b[0;32m     16\u001b[0m columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m feature_imp\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'rows'"
     ]
    }
   ],
   "source": [
    "#param_grid_rfc = {'n_estimators': [50, 100, 150, 200],\n",
    "#             'criterion': ['gini', 'entropy'],\n",
    "#             'max_features': ['sqrt', 'log2', None]}\n",
    "#rfcClassifier = GridSearchCV(rfcModel, param_grid_rfc)\n",
    "#rfcClassifier.fit(X_train, y_train)\n",
    "\n",
    "# Hyperparameters for Best Performing Model\n",
    "#for key, value in rfcClassifier.best_params_.items():\n",
    "#    print(f\"Hyperparameter: {key}; Value: {value}\")\n",
    "\n",
    "#rfcPipeline.steps[3][1].feature_importances_\n",
    "#rfcPipeline.steps[3][1].get_feature_names()\n",
    "\n",
    "#rfcPipeline.steps[3][1].fit(X_train, y_train)\n",
    "feature_imp = pd.DataFrame(rfcPipeline.steps[3][1].feature_importances_, index=X_train.to_frame().rows,\n",
    "columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55335a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rfc = rfcClassifier.best_estimator_.predict(X_test)\n",
    "print(f\"Accuracy: {round( metrics.accuracy_score(y_test, pred_rfc) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, rfcClassifier.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rfcClassifier.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6fb84",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eda38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knnPipeline = train_test_analyze(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927771b",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Machine (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032f843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbmPipeline = train_test_analyze(GradientBoostingClassifier())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
